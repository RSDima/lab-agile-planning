@conference{290,
  keywords = {lightly-supervised training, unsupervised training, automatic transcription generation, audio harvesting, English, isiZulu},
  author = {Neil Kleynhans and Febe De Wet and Etienne Barnard},
  title = {Unsupervised acoustic model training: comparing South African English and isiZulu},
  abstract = {Large amounts of untranscribed audio data are generated every day. These audio resources can be used to develop robust acoustic models that can be used in a variety of speech-based systems. Manually transcribing this data is resource intensive and requires funding, time and expertise. Lightly-supervised training techniques, however, provide a means to rapidly transcribe audio, thus reducing the initial resource investment to begin the modelling process. Our findings suggest that the lightly-supervised training technique works well for English but when moving to an agglutinative language, such as isiZulu, the process fails to achieve the performance seen for English. Additionally, phone-based performances are significantly worse when compared to an approach using word-based language models. These results indicate a strong dependence on large or well-matched text resources for lightly-supervised training techniques.},
  year = {2015},
  journal = {Pattern Recognition Association of South Africa (PRASA)},
  chapter = {136-141},
  address = {Port Elizabeth, South Africa},
  isbn = {978-1-4673-7450-7, 978-1-4673-7449-1},
  doi = { 10.1109/RoboMech.2015.7359512},
}

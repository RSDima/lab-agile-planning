@article{391,
  keywords = {Deep learning, Machine learning, Learning theory, Generalisation},
  author = {Marelie Davel},
  title = {Using summary layers to probe neural network behaviour},
  abstract = {No framework exists that can explain and predict the generalisation ability of deep neural networks in general circumstances. In fact, this question has not been answered for some of the least complicated of neural network architectures: fully-connected feedforward networks with rectified linear activations and a limited number of hidden layers. For such an architecture, we show how adding a summary layer to the network makes it more amenable to analysis, and allows us to define the conditions that are required to guarantee that a set of samples will all be classified correctly. This process does not describe the generalisation behaviour of these networks,but produces a number of metrics that are useful for probing their learning and generalisation behaviour. We support the analytical conclusions with empirical results, both to confirm that the mathematical guarantees hold in practice, and to demonstrate the use of the analysis process.},
  year = {2020},
  journal = {South African Computer Journal},
  volume = {32},
  chapter = {102-123},
  publisher = {South African Institute of Computer Scientists and Information Technologists},
  isbn = {ISSN: 1015-7999; E:2313-7835},
  url = {http://hdl.handle.net/10394/36916},
  doi = {https://doi.org/10.18489/sacj.v32i2.861},
}

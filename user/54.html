<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="content: http://purl.org/rss/1.0/modules/content/  dc: http://purl.org/dc/terms/  foaf: http://xmlns.com/foaf/0.1/  og: http://ogp.me/ns#  rdfs: http://www.w3.org/2000/01/rdf-schema#  schema: http://schema.org/  sioc: http://rdfs.org/sioc/ns#  sioct: http://rdfs.org/sioc/types#  skos: http://www.w3.org/2004/02/skos/core#  xsd: http://www.w3.org/2001/XMLSchema# ">
  <head>
    <meta charset="utf-8">
<noscript><style>form.antibot * :not(.antibot-message) { display: none !important; }</style>
</noscript><meta name="Generator" content="Drupal 9 (https://www.drupal.org)">
<meta name="MobileOptimized" content="width">
<meta name="HandheldFriendly" content="true">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link rel="stylesheet" href="../sites/default/files/fontyourface/local_fonts/muli_regular/font.css" media="all">
<link rel="stylesheet" href="../sites/default/files/fontyourface/local_fonts/muli_semibold/font.css" media="all">
<link rel="stylesheet" href="../sites/default/files/fontyourface/local_fonts/open_sans_regular/font.css" media="all">
<link rel="stylesheet" href="../sites/default/files/fontyourface/local_fonts/open_sans_bold/font.css" media="all">
<link rel="stylesheet" href="../sites/default/files/fontyourface/local_fonts/muli_light/font.css" media="all">
<link rel="stylesheet" href="../sites/default/files/fontyourface/local_fonts/open_sans_semibold/font.css" media="all">
<link rel="stylesheet" href="../sites/default/files/fontyourface/local_fonts/open_sans_light/font.css" media="all">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta about="/user/54" property="schema:name" content="Marelie Davel" lang="">
<link rel="icon" href="../themes/custom/cair_sass/favicon.ico" type="image/vnd.microsoft.icon">
<link rel="canonical" href="54.html">
<link rel="shortlink" href="54.html">
<script>window.a2a_config=window.a2a_config||{};a2a_config.callbacks=[];a2a_config.overlays=[];a2a_config.templates={};</script>

    <title>Marelie Davel | CAIR</title>
    <link rel="stylesheet" media="all" href="../core/modules/system/css/components/ajax-progress.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/align.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/autocomplete-loading.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/fieldgroup.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/container-inline.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/clearfix.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/details.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/hidden.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/item-list.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/js.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/nowrap.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/position-container.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/progress.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/reset-appearance.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/resize.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/sticky-header.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/system-status-counter.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/system-status-report-counters.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/system-status-report-general-info.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/tabledrag.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/tablesort.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/system/css/components/tree-child.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../core/modules/views/css/views.module.css?s9x51z">
<link rel="stylesheet" media="all" href="../modules/contrib/blazy/css/blazy.css?s9x51z">
<link rel="stylesheet" media="all" href="../modules/contrib/addtoany/css/addtoany.css?s9x51z">
<link rel="stylesheet" media="all" href="../modules/custom/particlesjs/particlesjs_cair/css/particlesjs_cair.css?s9x51z">
<link rel="stylesheet" media="all" href="../themes/custom/cair_sass/css/style.css?s9x51z">
<link rel="stylesheet" media="all" href="../themes/custom/cair_sass/css/custom.css?s9x51z">
<link rel="stylesheet" media="all" href="../themes/contrib/bootstrap_barrio/css/components/breadcrumb.css?s9x51z">
<link rel="stylesheet" media="all" href="../themes/contrib/bootstrap_barrio/css/components/form.css?s9x51z">
<link rel="stylesheet" media="all" href="../themes/contrib/gin/dist/css/components/ajax.css?s9x51z">

    
  </head>
  <body class="fontyourface layout-no-sidebars page-user-54 path-user">
    <a href="#main-content" class="visually-hidden focusable skip-link">
      Skip to main content
    </a>
    
      <div class="dialog-off-canvas-main-canvas" data-off-canvas-main-canvas="">
    <div id="page-wrapper">
  <div id="page">
    <header id="header" class="header" role="banner" aria-label="Site header">
                      <nav class="navbar navbar-light bg-light navbar-expand-lg" id="navbar-main">
                    <div class="container">
                              <a href="../index.htm" title="Home" rel="home" class="navbar-brand">
            <span class="ml-2 d-none d-md-inline">CAIR</span>
    </a>
    

                          <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#CollapsingNavbar" aria-controls="CollapsingNavbar" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
              <div class="collapse navbar-collapse" id="CollapsingNavbar">
                  <nav role="navigation" aria-labelledby="block-cair-sass-main-menu-menu" id="block-cair-sass-main-menu" class="block block-menu navigation menu--main">
            
  <h2 class="sr-only" id="block-cair-sass-main-menu-menu">Main navigation</h2>
  

        
              <ul block="block-cair-sass-main-menu" class="clearfix nav navbar-nav">
                    <li class="nav-item">
                          <a href="../group.html" class="nav-link nav-link--group" data-drupal-link-system-path="node/18">Research Groups</a>
              </li>
                <li class="nav-item">
                          <a href="../people.html" class="nav-link nav-link--people" data-drupal-link-system-path="node/23">People</a>
              </li>
                <li class="nav-item">
                          <a href="../research-publications.html" class="nav-link nav-link--research-publications" data-drupal-link-system-path="node/22">Research Publications</a>
              </li>
                <li class="nav-item">
                          <a href="../about.html" class="nav-link nav-link--about" data-drupal-link-system-path="node/4">About</a>
              </li>
                <li class="nav-item">
                          <a href="../contact.html" class="nav-link nav-link--contact" data-drupal-link-system-path="webform/contact">Contact</a>
              </li>
        </ul>
  


  </nav>
<nav role="navigation" aria-labelledby="block-searchmenu-menu" id="block-searchmenu" class="block block-menu navigation menu--search-menu">
            
  <h2 class="sr-only" id="block-searchmenu-menu">Search menu</h2>
  

        
              <ul block="block-searchmenu" class="clearfix nav">
                    <li class="nav-item">
                
        <a href="../search.html" class="nav-item nav-link">
            <span>Search</span>
        </a>

              </li>
        </ul>
  


  </nav>


                	          </div>
            
                          <div class="search-expandable px-3 z-10 invisible">
                <div class="d-flex align-items-center h-100 justify-content-center mw-760 mx-auto">  <div class="views-exposed-form block block-views block-views-exposed-filter-blocksearch-api-database-search-page-1" data-drupal-selector="views-exposed-form-search-api-database-search-page-1" id="block-exposedformsearch-api-database-searchpage-1">
  
    
      <div class="content">
      
      <div class="form-row">
        
<form action="/search" method="get" id="views-exposed-form-search-api-database-search-page-1" accept-charset="UTF-8">
  <div class="form-row">
  



  <fieldset class="js-form-item js-form-type-textfield form-type-textfield js-form-item-search-api-fulltext form-item-search-api-fulltext form-no-label form-group">
                    <input placeholder="Search for content or users" data-drupal-selector="edit-search-api-fulltext" type="text" id="edit-search-api-fulltext" name="search_api_fulltext" value="" size="30" maxlength="128" class="form-text form-control">

                      </fieldset>
<div data-drupal-selector="edit-actions" class="form-actions js-form-wrapper form-group col-auto" id="edit-actions"><input data-drupal-selector="edit-submit-search-api-database-search" type="submit" id="edit-submit-search-api-database-search" value="Apply" class="button js-form-submit form-submit btn btn-primary form-control">
</div>

</div>

</form>

        <div class="col">
          <button type="reset" class="btn btn-link search-reset">Close</button>
        </div>
      </div>

    </div>
  </div>

</div>
              </div>
                        
                                </div>
                  </nav>
          </header>
                <div class="breadcrumb-wrapper position-relative d-flex align-items-center fade-in">
      <div class="container">  <div id="block-cair-sass-breadcrumbs" class="block block-system block-system-breadcrumb-block">
  
    
      <div class="content">
      

  <nav role="navigation" aria-label="breadcrumb">
    <ol class="breadcrumb">
                  <li class="breadcrumb-item">
          <a href="../index.htm">Home</a>
        </li>
              </ol>
  </nav>

    </div>
  </div>

</div>
        <section class="row region region-breadcrumb-bg">
    <div id="block-particlesjsexampleblock" class="block block-particlesjs-cair block-particlesjsexample">
  
    
       <div id="particles-js" class="h-100"></div>
  </div>

  </section>

    </div>
    
    <div id="main-wrapper" class="layout-main-wrapper clearfix pt-3">
                    <div id="main" class="container">
          <div class="row row-offcanvas row-offcanvas-left clearfix">
              <main class="main-content col" id="content" role="main">
                <section class="section">
                  <a id="main-content" tabindex="-1"></a>
                    <div data-drupal-messages-fallback="" class="hidden"></div>
<div id="block-cair-sass-content" class="block block-system block-system-main-block">
  
    
      <div class="content">
      <article typeof="schema:Person" about="/user/54" class="profile">
  </article>

    </div>
  </div>
<div class="views-element-container block block-views block-views-blockuser-main-profile-block-1" id="block-views-block-user-main-profile-block-1">
  
    
      <div class="content">
      <div class="col-auto"><div class="view view-user-main-profile view-id-user_main_profile view-display-id-block_1 js-view-dom-id-08937de8d9096e60576e973c937f2feaed6e8c7f4a2ca50130d00b379a0cefc1">
  
    
      <div class="view-header">
      <div class="row">
<div class="col">
<div><h1 class="title">Marelie Davel</h1></div>
</div>
</div>
    </div>
      
      <div class="view-content row">
          <div class="w-100 views-row">
    
<div class="row mb-3">
	<div class="mb-2 col-sm-2 text-center img-fluid">  <img src="../sites/default/files/styles/user/public/pictures/2019-11/Marelie%20Davel.jpg?itok=LsoMLVFz" width="80" height="120" alt="" loading="lazy" typeof="foaf:Image" class="image-style-user">


</div>
	<div class="mb-2 col">
					<p>Prof</p>
											<div class="mb-2">
				<span>Marelie Davel is a Research Professor in the Faculty of Engineering of North-West University (NWU), and the Director of the Multilingual Speech Technologies (MuST) research group. Building on a long-standing interest in the theory and application of machine learning, her current research focuses on the generalisation ability of deep neural networks.<br>
 <br>
Marelie obtained her undergraduate degrees (Computer Science &amp; Mathematics) from Stellenbosch University, receiving the Dean’s medal as best student in the US Faculty of Science at the end of her Honours degree. Prior to joining NWU, Marelie was a principal researcher and research group leader at the South African CSIR, involved in technology-oriented research and development. Her research group focussed on speech technology development in under-resourced environments; in 2005, she received her PhD from the University of Pretoria (UP), with a thesis on bootstrapping pronunciation models, at the time one of the core ‘missing’ components when developing speech recognition for South African languages. <br>
 <br>
In 2011, Marelie joined NWU, becoming the Director of MuST in 2014. MuST is a focussed research environment with an emphasis on postgraduate training and delivering on externally-focussed projects. Recent projects include the development of an automatic speech transcription platform for the South African government, development of a new multilingual text-to-speech corpus in collaboration with Internet giant Google, and being part of the winning consortium of the BABEL project: a 5-year internationally collaborative challenge aimed at solving the spoken term detection task for under-resourced languages. <br>
 <br>
Over the past few years, Marelie has supervised 23 post-graduate students, all producing research related to the theory and applications of machine learning. She frequently participates in various scientific committees both nationally and internationally (AAAI, IJCAI, Interspeech, SLT, MediaEval, ICASSP, SLTU), is the NWU group representative at the national Centre for Artificial Intelligence Research (CAIR), and an NRF-rated researcher. Since 2003, she has published 100 peer-reviewed papers related to machine learning; she has an h-index of 21, and an i10-index of 37.</span>
			</div>
					</div>
	<div class="uni col-sm-2 text-center mx-auto">
		<div class="img-fluid px-2">  <img src="../sites/default/files/styles/user/public/2019-07/nwu-logo-purple.png?itok=EvSj5Xbl" width="120" height="83" alt="North West University" loading="lazy" typeof="foaf:Image" class="image-style-user">


</div>
	</div>
</div>



  </div>

    </div>
  
          </div>
</div>

    </div>
  </div>


                </section>
              </main>
                                  </div>
        </div>
          </div>
          <div class="featured-bottom py-5">
        <aside class="container clearfix" role="complementary">
          <div class="row">
            <div class="col-auto">  <div class="views-element-container card p-3 listing-bordered block block-views block-views-blockresearch-papers-block-2" id="block-views-block-research-papers-block-2">
  
      <h2>Publications</h2>
    
      <div class="content">
      <div class="col-auto"><div class="view view-research-papers view-id-research_papers view-display-id-block_2 js-view-dom-id-f843f2bbb0cbc8f7c59a20ed5ca5da5bf383e309aea77bed48a3aebbf98c560a">
  
    
      
      <div class="view-content row">
      <div>
  <h3 class="mb-3">2022</h3>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-492">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Heymans W, Davel MH, Van Heerden CJ. Efficient acoustic feature transformation in mismatched environments using a Guided-GAN. <i>Speech Communication</i>. 2022;143. doi:https://doi.org/10.1016/j.specom.2022.07.002.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-492" role="button" aria-expanded="false" aria-controls="collapseAbstract-492">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-492" role="button" aria-expanded="false" aria-controls="collapseBibtext-492">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-492-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                 
    </div>

    <div class="collapse" data-parent="#row-492" id="collapsePdf-492">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-492" id="collapseAbstract-492">
        <div class="p-3 border-left-grey mt-2">
            <p>We propose a new framework to improve automatic speech recognition (ASR) systems in resource-scarce environments using a generative adversarial network (GAN) operating on acoustic input features. The GAN is used to enhance the features of mismatched data prior to decoding, or can optionally be used to fine-tune the acoustic model. We achieve improvements that are comparable to multi-style training (MTR), but at a lower computational cost. With less than one hour of data, an ASR system trained on good quality data, and evaluated on mismatched audio is improved by between 11.5% and 19.7% relative word error rate (WER). Experiments demonstrate that the framework can be very useful in under-resourced environments where training data and computational resources are limited. The GAN does not require parallel training data, because it utilises a baseline acoustic model to provide an additional loss term that guides the generator to create acoustic features that are better classified by the baseline.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-492" id="collapseBibtext-492">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;492,
&nbsp;&nbsp;author = &#123;Walter Heymans and Marelie Davel and Charl Van Heerden&#125;,
&nbsp;&nbsp;title = &#123;Efficient acoustic feature transformation in mismatched environments using a Guided-GAN&#125;,
&nbsp;&nbsp;abstract = &#123;We propose a new framework to improve automatic speech recognition (ASR) systems in resource-scarce environments using a generative adversarial network (GAN) operating on acoustic input features. The GAN is used to enhance the features of mismatched data prior to decoding, or can optionally be used to fine-tune the acoustic model. We achieve improvements that are comparable to multi-style training (MTR), but at a lower computational cost. With less than one hour of data, an ASR system trained on good quality data, and evaluated on mismatched audio is improved by between 11.5% and 19.7% relative word error rate (WER). Experiments demonstrate that the framework can be very useful in under-resourced environments where training data and computational resources are limited. The GAN does not require parallel training data, because it utilises a baseline acoustic model to provide an additional loss term that guides the generator to create acoustic features that are better classified by the baseline.&#125;,
&nbsp;&nbsp;year = &#123;2022&#125;,
&nbsp;&nbsp;journal = &#123;Speech Communication&#125;,
&nbsp;&nbsp;volume = &#123;143&#125;,
&nbsp;&nbsp;pages = &#123;10 - 20&#125;,
&nbsp;&nbsp;month = &#123;09/2022&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.1016/j.specom.2022.07.002&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">

<div class="mb-20" id="row-491">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Oosthuizen AJ, Davel MH, Helberg A. Multi-Layer Perceptron for Channel State Information Estimation: Design Considerations. In: <i>Southern Africa Telecommunication Networks and Applications Conference (SATNAC)</i>. Fancourt, George; 2022.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-491" role="button" aria-expanded="false" aria-controls="collapseAbstract-491">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-491" role="button" aria-expanded="false" aria-controls="collapseBibtext-491">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-491-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-491" role="button" aria-expanded="false" aria-controls="collapsePdf-491">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-491" id="collapsePdf-491">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2022-09/SATNAC2022%20Multi-Layer%20Perceptron%20for%20Channel%20State%20Information%20Estimation%20Design%20Considerations.pdf" type="application/pdf">SATNAC2022 Multi-Layer Perceptron for Channel State Information Estimation Design Considerations.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-491" id="collapseAbstract-491">
        <div class="p-3 border-left-grey mt-2">
            <p>The accurate estimation of channel state information (CSI) is an important aspect of wireless communications. In this paper, a multi-layer perceptron (MLP) is developed as a CSI estimator in long-term evolution (LTE) transmission conditions. The representation of the CSI data is investigated in conjunction with batch normalisation and the representational ability of MLPs. It is found that discontinuities in the representational feature space can cripple an MLP’s ability to accurately predict CSI when noise is present. Different ways in which to mitigate this effect are analysed and a solution developed, initially in the context of channels that are only affected by additive white
Guassian noise. The developed architecture is then applied to more complex channels with various delay profiles and Doppler spread. The performance of the proposed MLP is shown to be comparable with LTE minimum mean squared error (MMSE), and to outperform least square (LS) estimation over a range of channel conditions.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-491" id="collapseBibtext-491">
<div class="p-3 border-left-grey mt-2">
<pre>@&#123;491,
&nbsp;&nbsp;author = &#123;Andrew Oosthuizen and Marelie Davel and Albert Helberg&#125;,
&nbsp;&nbsp;title = &#123;Multi-Layer Perceptron for Channel State Information Estimation: Design Considerations&#125;,
&nbsp;&nbsp;abstract = &#123;The accurate estimation of channel state information (CSI) is an important aspect of wireless communications. In this paper, a multi-layer perceptron (MLP) is developed as a CSI estimator in long-term evolution (LTE) transmission conditions. The representation of the CSI data is investigated in conjunction with batch normalisation and the representational ability of MLPs. It is found that discontinuities in the representational feature space can cripple an MLP’s ability to accurately predict CSI when noise is present. Different ways in which to mitigate this effect are analysed and a solution developed, initially in the context of channels that are only affected by additive white
Guassian noise. The developed architecture is then applied to more complex channels with various delay profiles and Doppler spread. The performance of the proposed MLP is shown to be comparable with LTE minimum mean squared error (MMSE), and to outperform least square (LS) estimation over a range of channel conditions.&#125;,
&nbsp;&nbsp;year = &#123;2022&#125;,
&nbsp;&nbsp;journal = &#123;Southern Africa Telecommunication Networks and Applications Conference (SATNAC)&#125;,
&nbsp;&nbsp;pages = &#123;94 - 99&#125;,
&nbsp;&nbsp;month = &#123;08/2022&#125;,
&nbsp;&nbsp;address = &#123;Fancourt, George&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-483">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Modipa T, Davel MH. Two Sepedi‑English code‑switched speech corpora. <i>Language Resources and Evaluation</i>. 2022;56. doi:https://doi.org/10.1007/s10579-022-09592-6 (Read here: https://rdcu.be/cO6lD).</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-483" role="button" aria-expanded="false" aria-controls="collapseAbstract-483">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-483" role="button" aria-expanded="false" aria-controls="collapseBibtext-483">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-483-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                 
    </div>

    <div class="collapse" data-parent="#row-483" id="collapsePdf-483">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-483" id="collapseAbstract-483">
        <div class="p-3 border-left-grey mt-2">
            <p>We report on the development of two reference corpora for the analysis of SepediEnglish code-switched speech in the context of automatic speech recognition. For the first corpus, possible English events were obtained from an existing corpus of transcribed Sepedi-English speech. The second corpus is based on the analysis of radio broadcasts: actual instances of code switching were transcribed and reproduced by a number of native Sepedi speakers. We describe the process to develop and verify both corpora and perform an initial analysis of the newly produced data sets. We find that, in naturally occurring speech, the frequency of code switching is unexpectedly high for this language pair, and that the continuum of code switching (from unmodified embedded words to loanwords absorbed into the matrix language) makes this a particularly challenging task for speech recognition systems.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-483" id="collapseBibtext-483">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;483,
&nbsp;&nbsp;author = &#123;Thipe Modipa and Marelie Davel&#125;,
&nbsp;&nbsp;title = &#123;Two Sepedi‑English code‑switched speech corpora&#125;,
&nbsp;&nbsp;abstract = &#123;We report on the development of two reference corpora for the analysis of SepediEnglish code-switched speech in the context of automatic speech recognition. For the first corpus, possible English events were obtained from an existing corpus of transcribed Sepedi-English speech. The second corpus is based on the analysis of radio broadcasts: actual instances of code switching were transcribed and reproduced by a number of native Sepedi speakers. We describe the process to develop and verify both corpora and perform an initial analysis of the newly produced data sets. We find that, in naturally occurring speech, the frequency of code switching is unexpectedly high for this language pair, and that the continuum of code switching (from unmodified embedded words to loanwords absorbed into the matrix language) makes this a particularly challenging task for speech recognition systems.&#125;,
&nbsp;&nbsp;year = &#123;2022&#125;,
&nbsp;&nbsp;journal = &#123;Language Resources and Evaluation&#125;,
&nbsp;&nbsp;volume = &#123;56&#125;,
&nbsp;&nbsp;pages = &#123;https://rdcu.be/cO6lD)&#125;,
&nbsp;&nbsp;publisher = &#123;Springer&#125;,
&nbsp;&nbsp;address = &#123;South Africa&#125;,
&nbsp;&nbsp;url = &#123;https://rdcu.be/cO6lD&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.1007/s10579-022-09592-6 (Read here: https://rdcu.be/cO6lD)&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-480">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Heymans W, Davel MH, Van Heerden CJ. Multi-style Training for South African Call Centre Audio. <i>Communications in Computer and Information Science</i>. 2022;1551. doi:https://doi.org/10.1007/978-3-030-95070-5_8.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-480" role="button" aria-expanded="false" aria-controls="collapseAbstract-480">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-480" role="button" aria-expanded="false" aria-controls="collapseBibtext-480">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-480-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-480" role="button" aria-expanded="false" aria-controls="collapsePdf-480">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-480" id="collapsePdf-480">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2022-06/Multi-style%20Training%20for%20South%20African%20Call%20Centre%20Audio.pdf" type="application/pdf">Multi-style Training for South African Call Centre Audio.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-480" id="collapseAbstract-480">
        <div class="p-3 border-left-grey mt-2">
            <p>Mismatched data is a challenging problem for automatic speech recognition (ASR) systems. One of the most common techniques used to address mismatched data is multi-style training (MTR), a form of data augmentation that attempts to transform the training data to be more representative of the testing data; and to learn robust representations applicable to different conditions. This task can be very challenging if the test conditions are unknown. We explore the impact of different MTR styles on system performance when testing conditions are different from training conditions in the context of deep neural network hidden Markov model (DNN-HMM) ASR systems. A controlled environment is created using the LibriSpeech corpus, where we isolate the effect of different MTR styles on final system performance. We evaluate our findings on a South African call centre dataset that contains noisy, WAV49-encoded audio.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-480" id="collapseBibtext-480">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;480,
&nbsp;&nbsp;author = &#123;Walter Heymans and Marelie Davel and Charl Van Heerden&#125;,
&nbsp;&nbsp;title = &#123;Multi-style Training for South African Call Centre Audio&#125;,
&nbsp;&nbsp;abstract = &#123;Mismatched data is a challenging problem for automatic speech recognition (ASR) systems. One of the most common techniques used to address mismatched data is multi-style training (MTR), a form of data augmentation that attempts to transform the training data to be more representative of the testing data; and to learn robust representations applicable to different conditions. This task can be very challenging if the test conditions are unknown. We explore the impact of different MTR styles on system performance when testing conditions are different from training conditions in the context of deep neural network hidden Markov model (DNN-HMM) ASR systems. A controlled environment is created using the LibriSpeech corpus, where we isolate the effect of different MTR styles on final system performance. We evaluate our findings on a South African call centre dataset that contains noisy, WAV49-encoded audio.&#125;,
&nbsp;&nbsp;year = &#123;2022&#125;,
&nbsp;&nbsp;journal = &#123;Communications in Computer and Information Science&#125;,
&nbsp;&nbsp;volume = &#123;1551&#125;,
&nbsp;&nbsp;pages = &#123;111 - 124&#125;,
&nbsp;&nbsp;publisher = &#123;Southern African Conference for Artificial Intelligence Research&#125;,
&nbsp;&nbsp;address = &#123;South Africa&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.1007/978-3-030-95070-5_8&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-479">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Mouton C, Davel MH. Exploring layerwise decision making in DNNs. <i>Communications in Computer and Information Science</i>. 2022;1551. doi:https://doi.org/10.1007/978-3-030-95070-5_10.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-479" role="button" aria-expanded="false" aria-controls="collapseAbstract-479">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-479" role="button" aria-expanded="false" aria-controls="collapseBibtext-479">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-479-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-479" role="button" aria-expanded="false" aria-controls="collapsePdf-479">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-479" id="collapsePdf-479">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2022-06/Exploring%20layerwise%20decision%20making%20in%20DNNs.pdf" type="application/pdf">Exploring layerwise decision making in DNNs.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-479" id="collapseAbstract-479">
        <div class="p-3 border-left-grey mt-2">
            <p>While deep neural networks (DNNs) have become a standard architecture for many machine learning tasks, their internal decision-making process and general interpretability is still poorly understood. Conversely, common decision trees are easily interpretable and theoretically well understood. We show that by encoding the discrete sample activation values of nodes as a binary representation, we are able to extract a decision tree explaining the classification procedure of each layer in a ReLU-activated multilayer perceptron (MLP). We then combine these decision trees with existing feature attribution techniques in order to produce an interpretation of each layer of a model. Finally, we provide an analysis of the generated interpretations, the behaviour of the binary encodings and how these relate to sample groupings created during the training process of the neural network.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-479" id="collapseBibtext-479">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;479,
&nbsp;&nbsp;author = &#123;Coenraad Mouton and Marelie Davel&#125;,
&nbsp;&nbsp;title = &#123;Exploring layerwise decision making in DNNs&#125;,
&nbsp;&nbsp;abstract = &#123;While deep neural networks (DNNs) have become a standard architecture for many machine learning tasks, their internal decision-making process and general interpretability is still poorly understood. Conversely, common decision trees are easily interpretable and theoretically well understood. We show that by encoding the discrete sample activation values of nodes as a binary representation, we are able to extract a decision tree explaining the classification procedure of each layer in a ReLU-activated multilayer perceptron (MLP). We then combine these decision trees with existing feature attribution techniques in order to produce an interpretation of each layer of a model. Finally, we provide an analysis of the generated interpretations, the behaviour of the binary encodings and how these relate to sample groupings created during the training process of the neural network.&#125;,
&nbsp;&nbsp;year = &#123;2022&#125;,
&nbsp;&nbsp;journal = &#123;Communications in Computer and Information Science&#125;,
&nbsp;&nbsp;volume = &#123;1551&#125;,
&nbsp;&nbsp;pages = &#123;140 - 155&#125;,
&nbsp;&nbsp;publisher = &#123;Artificial Intelligence Research (SACAIR 2021)&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.1007/978-3-030-95070-5_10&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
</div>
<div>
  <h3 class="mb-3">2021</h3>
    <div class="w-100 views-row">

<div class="mb-20" id="row-482">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Van Wyk L, Davel MH, Van Heerden CJ. Unsupervised fine-tuning of speaker diarisation pipelines using silhouette coefficients. In: <i>Southern African Conference for Artificial Intelligence Research</i>. South Africa; 2021. https://2021.sacair.org.za/proceedings/.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-482" role="button" aria-expanded="false" aria-controls="collapseAbstract-482">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-482" role="button" aria-expanded="false" aria-controls="collapseBibtext-482">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-482-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-482" role="button" aria-expanded="false" aria-controls="collapsePdf-482">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-482" id="collapsePdf-482">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2022-06/Unsupervised%20fine-tuning%20of%20speaker%20diarisation%20pipelines%20using%20silhouette%20coefficients.pdf" type="application/pdf">Unsupervised fine-tuning of speaker diarisation pipelines using silhouette coefficients.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-482" id="collapseAbstract-482">
        <div class="p-3 border-left-grey mt-2">
            <p>We investigate the use of silhouette coefficients in cluster analysis for speaker diarisation, with the dual purpose of unsupervised fine-tuning during domain adaptation and determining the number of speakers in an audio file. Our main contribution is to demonstrate the use of silhouette coefficients to perform per-file domain adaptation, which we show to deliver an improvement over per-corpus domain adaptation. Secondly, we show that this method of silhouette-based cluster analysis can be used to accurately determine more than one hyperparameter at the same time. Finally, we propose a novel method for calculating the silhouette coefficient of clusters using a PLDA score matrix as input.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-482" id="collapseBibtext-482">
<div class="p-3 border-left-grey mt-2">
<pre>@&#123;482,
&nbsp;&nbsp;author = &#123;Lucas Van Wyk and Marelie Davel and Charl Van Heerden&#125;,
&nbsp;&nbsp;title = &#123;Unsupervised fine-tuning of speaker diarisation pipelines using silhouette coefficients&#125;,
&nbsp;&nbsp;abstract = &#123;We investigate the use of silhouette coefficients in cluster analysis for speaker diarisation, with the dual purpose of unsupervised fine-tuning during domain adaptation and determining the number of speakers in an audio file. Our main contribution is to demonstrate the use of silhouette coefficients to perform per-file domain adaptation, which we show to deliver an improvement over per-corpus domain adaptation. Secondly, we show that this method of silhouette-based cluster analysis can be used to accurately determine more than one hyperparameter at the same time. Finally, we propose a novel method for calculating the silhouette coefficient of clusters using a PLDA score matrix as input.&#125;,
&nbsp;&nbsp;year = &#123;2021&#125;,
&nbsp;&nbsp;journal = &#123;Southern African Conference for Artificial Intelligence Research&#125;,
&nbsp;&nbsp;pages = &#123;202 - 216&#125;,
&nbsp;&nbsp;month = &#123;06/12/2021 - 10/12/2021&#125;,
&nbsp;&nbsp;address = &#123;South Africa&#125;,
&nbsp;&nbsp;isbn = &#123;978-0-620-94410-6&#125;,
&nbsp;&nbsp;url = &#123;https://2021.sacair.org.za/proceedings/&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">

<div class="mb-20" id="row-481">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Oosthuizen AJ, Davel MH, Helberg A. Exploring CNN-based automatic modulation classification using small modulation sets. In: <i>Southern Africa Telecommunication Networks and Applications Conference</i>. South Africa; 2021. https://www.satnac.org.za/proceedings.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-481" role="button" aria-expanded="false" aria-controls="collapseAbstract-481">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-481" role="button" aria-expanded="false" aria-controls="collapseBibtext-481">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-481-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-481" role="button" aria-expanded="false" aria-controls="collapsePdf-481">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-481" id="collapsePdf-481">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2022-06/Exploring%20CNN-based%20automatic%20modulation%20classification%20using%20small%20modulation%20sets.pdf" type="application/pdf">Exploring CNN-based automatic modulation classification using small modulation sets.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-481" id="collapseAbstract-481">
        <div class="p-3 border-left-grey mt-2">
            <p>We investigate the effect of a reduced modulation scheme pool on a CNN-based automatic modulation classifier. Similar classifiers in literature are typically used to classify sets of five or more different modulation types [1] [2], whereas our
analysis is of a CNN classifier that classifies between two modulation types, 16-QAM and 8-PSK, only. While implementing the network, we observe that the network’s classification accuracy improves for lower SNR instead of reducing as expected. This analysis exposes characteristics of such classifiers that can be used to improve CNN classifiers on larger sets of modulation types. We show that presenting the SNR data as an extra data point to the network can significantly increase classification accuracy.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-481" id="collapseBibtext-481">
<div class="p-3 border-left-grey mt-2">
<pre>@&#123;481,
&nbsp;&nbsp;author = &#123;Andrew Oosthuizen and Marelie Davel and Albert Helberg&#125;,
&nbsp;&nbsp;title = &#123;Exploring CNN-based automatic modulation classification using small modulation sets&#125;,
&nbsp;&nbsp;abstract = &#123;We investigate the effect of a reduced modulation scheme pool on a CNN-based automatic modulation classifier. Similar classifiers in literature are typically used to classify sets of five or more different modulation types [1] [2], whereas our
analysis is of a CNN classifier that classifies between two modulation types, 16-QAM and 8-PSK, only. While implementing the network, we observe that the network’s classification accuracy improves for lower SNR instead of reducing as expected. This analysis exposes characteristics of such classifiers that can be used to improve CNN classifiers on larger sets of modulation types. We show that presenting the SNR data as an extra data point to the network can significantly increase classification accuracy.&#125;,
&nbsp;&nbsp;year = &#123;2021&#125;,
&nbsp;&nbsp;journal = &#123;Southern Africa Telecommunication Networks and Applications Conference&#125;,
&nbsp;&nbsp;pages = &#123;20 - 24&#125;,
&nbsp;&nbsp;month = &#123;21/11/2021 - 23/11/2021&#125;,
&nbsp;&nbsp;address = &#123;South Africa&#125;,
&nbsp;&nbsp;url = &#123;https://www.satnac.org.za/proceedings&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
</div>
<div>
  <h3 class="mb-3">2020</h3>
    <div class="w-100 views-row">

<div class="mb-20" id="row-402">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Haasbroek DG, Davel MH. Exploring neural network training dynamics through binary node activations. In: <i>Southern African Conference for Artificial Intelligence Research</i>. South Africa; 2020. https://sacair.org.za/proceedings/.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-402" role="button" aria-expanded="false" aria-controls="collapseAbstract-402">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-402" role="button" aria-expanded="false" aria-controls="collapseBibtext-402">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-402-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-402" role="button" aria-expanded="false" aria-controls="collapsePdf-402">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-402" id="collapsePdf-402">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2021-04/Haasbroek-2020-exploring-nn-training_0.pdf" type="application/pdf">Haasbroek-2020-exploring-nn-training_0.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-402" id="collapseAbstract-402">
        <div class="p-3 border-left-grey mt-2">
            <p>Each node in a neural network is trained to activate for a specific region in the input domain. Any training samples that fall within this domain are therefore implicitly clustered together. Recent work has highlighted the importance of these clusters during the training process but has not yet investigated their evolution during training. Towards this goal, we train several ReLU-activated MLPs on a simple classification task (MNIST) and show that a consistent training process emerges: (1) sample clusters initially increase in size and then decrease as training progresses, (2) the size of sample clusters in the first layer decreases more rapidly than in deeper layers, (3) binary node activations, especially of nodes in deeper layers, become more sensitive to class membership as training progresses, (4) individual nodes remain poor predictors of class membership, even if accurate when applied as a group. We report on the detail of these findings and interpret them from the perspective of a high-dimensional clustering process.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-402" id="collapseBibtext-402">
<div class="p-3 border-left-grey mt-2">
<pre>@&#123;402,
&nbsp;&nbsp;author = &#123;Daniël Haasbroek and Marelie Davel&#125;,
&nbsp;&nbsp;title = &#123;Exploring neural network training dynamics through binary node activations&#125;,
&nbsp;&nbsp;abstract = &#123;Each node in a neural network is trained to activate for a specific region in the input domain. Any training samples that fall within this domain are therefore implicitly clustered together. Recent work has highlighted the importance of these clusters during the training process but has not yet investigated their evolution during training. Towards this goal, we train several ReLU-activated MLPs on a simple classification task (MNIST) and show that a consistent training process emerges: (1) sample clusters initially increase in size and then decrease as training progresses, (2) the size of sample clusters in the first layer decreases more rapidly than in deeper layers, (3) binary node activations, especially of nodes in deeper layers, become more sensitive to class membership as training progresses, (4) individual nodes remain poor predictors of class membership, even if accurate when applied as a group. We report on the detail of these findings and interpret them from the perspective of a high-dimensional clustering process.&#125;,
&nbsp;&nbsp;year = &#123;2020&#125;,
&nbsp;&nbsp;journal = &#123;Southern African Conference for Artificial Intelligence Research&#125;,
&nbsp;&nbsp;pages = &#123;304-320&#125;,
&nbsp;&nbsp;month = &#123;22/02/2021 - 26/02/2021&#125;,
&nbsp;&nbsp;address = &#123;South Africa&#125;,
&nbsp;&nbsp;isbn = &#123;978-0-620-89373-2&#125;,
&nbsp;&nbsp;url = &#123;https://sacair.org.za/proceedings/&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-484">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Venter AE, Theunissen MW, Davel MH. Pre-interpolation loss behaviour in neural networks. <i>Communications in Computer and Information Science</i>. 2020;1342. doi:https://doi.org/10.1007/978-3-030-66151-9_19.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-484" role="button" aria-expanded="false" aria-controls="collapseAbstract-484">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-484" role="button" aria-expanded="false" aria-controls="collapseBibtext-484">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-484-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-484" role="button" aria-expanded="false" aria-controls="collapsePdf-484">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-484" id="collapsePdf-484">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2021-04/Pre-interpolation-loss-behaviour-venter_0.pdf" type="application/pdf">Pre-interpolation-loss-behaviour-venter_0.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-484" id="collapseAbstract-484">
        <div class="p-3 border-left-grey mt-2">
            <p>When training neural networks as classifiers, it is common to observe an increase in average test loss while still maintaining or improving the overall classification accuracy on the same dataset. In spite of the ubiquity of this phenomenon, it has not been well studied and is often dismissively attributed to an increase in borderline correct classifications. We present an empirical investigation that shows how this phenomenon is actually a result of the differential manner by which test samples are processed. In essence: test loss does not increase overall, but only for a small minority of samples. Large representational capacities allow losses to decrease for the vast majority of test samples at the cost of extreme increases for others. This effect seems to be mainly caused by increased parameter values relating to the correctly processed sample features. Our findings contribute to the practical understanding of a common behaviour of deep neural networks. We also discuss the implications of this work for network optimisation and generalisation.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-484" id="collapseBibtext-484">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;484,
&nbsp;&nbsp;author = &#123;Arthur Venter and Marthinus Theunissen and Marelie Davel&#125;,
&nbsp;&nbsp;title = &#123;Pre-interpolation loss behaviour in neural networks&#125;,
&nbsp;&nbsp;abstract = &#123;When training neural networks as classifiers, it is common to observe an increase in average test loss while still maintaining or improving the overall classification accuracy on the same dataset. In spite of the ubiquity of this phenomenon, it has not been well studied and is often dismissively attributed to an increase in borderline correct classifications. We present an empirical investigation that shows how this phenomenon is actually a result of the differential manner by which test samples are processed. In essence: test loss does not increase overall, but only for a small minority of samples. Large representational capacities allow losses to decrease for the vast majority of test samples at the cost of extreme increases for others. This effect seems to be mainly caused by increased parameter values relating to the correctly processed sample features. Our findings contribute to the practical understanding of a common behaviour of deep neural networks. We also discuss the implications of this work for network optimisation and generalisation.&#125;,
&nbsp;&nbsp;year = &#123;2020&#125;,
&nbsp;&nbsp;journal = &#123;Communications in Computer and Information Science&#125;,
&nbsp;&nbsp;volume = &#123;1342&#125;,
&nbsp;&nbsp;pages = &#123;296-309&#125;,
&nbsp;&nbsp;publisher = &#123;Southern African Conference for Artificial Intelligence Research&#125;,
&nbsp;&nbsp;address = &#123;South Africa&#125;,
&nbsp;&nbsp;isbn = &#123;978-3-030-66151-9&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.1007/978-3-030-66151-9_19&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-485">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Myburgh JC, Mouton C, Davel MH. Tracking translation invariance in CNNs. <i>Communications in Computer and Information Science</i>. 2020;1342. doi:https://doi.org/10.1007/978-3-030-66151-9_18.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-485" role="button" aria-expanded="false" aria-controls="collapseAbstract-485">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-485" role="button" aria-expanded="false" aria-controls="collapseBibtext-485">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-485-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-485" role="button" aria-expanded="false" aria-controls="collapsePdf-485">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-485" id="collapsePdf-485">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2021-04/TrackingTranslationInvariance_Myburgh.pdf" type="application/pdf">TrackingTranslationInvariance_Myburgh.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-485" id="collapseAbstract-485">
        <div class="p-3 border-left-grey mt-2">
            <p>Although Convolutional Neural Networks (CNNs) are widely used, their translation invariance (ability to deal with translated inputs) is still subject to some controversy. We explore this question using translation-sensitivity maps to quantify how sensitive a standard CNN is to a translated input. We propose the use of cosine similarity as sensitivity metric over Euclidean distance, and discuss the importance of restricting the dimensionality of either of these metrics when comparing architectures. Our main focus is to investigate the effect of different architectural components of a standard CNN on that network’s sensitivity to translation. By varying convolutional kernel sizes and amounts of zero padding, we control the size of the feature maps produced, allowing us to quantify the extent to which these elements influence translation invariance. We also measure translation invariance at different locations within the CNN to determine the extent to which convolutional and fully connected layers, respectively, contribute to the translation invariance of a CNN as a whole. Our analysis indicates that both convolutional kernel size and feature map size have a systematic influence on translation invariance. We also see that convolutional layers contribute less than expected to translation invariance, when not specifically forced to do so.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-485" id="collapseBibtext-485">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;485,
&nbsp;&nbsp;author = &#123;Johannes Myburgh and Coenraad Mouton and Marelie Davel&#125;,
&nbsp;&nbsp;title = &#123;Tracking translation invariance in CNNs&#125;,
&nbsp;&nbsp;abstract = &#123;Although Convolutional Neural Networks (CNNs) are widely used, their translation invariance (ability to deal with translated inputs) is still subject to some controversy. We explore this question using translation-sensitivity maps to quantify how sensitive a standard CNN is to a translated input. We propose the use of cosine similarity as sensitivity metric over Euclidean distance, and discuss the importance of restricting the dimensionality of either of these metrics when comparing architectures. Our main focus is to investigate the effect of different architectural components of a standard CNN on that network’s sensitivity to translation. By varying convolutional kernel sizes and amounts of zero padding, we control the size of the feature maps produced, allowing us to quantify the extent to which these elements influence translation invariance. We also measure translation invariance at different locations within the CNN to determine the extent to which convolutional and fully connected layers, respectively, contribute to the translation invariance of a CNN as a whole. Our analysis indicates that both convolutional kernel size and feature map size have a systematic influence on translation invariance. We also see that convolutional layers contribute less than expected to translation invariance, when not specifically forced to do so.&#125;,
&nbsp;&nbsp;year = &#123;2020&#125;,
&nbsp;&nbsp;journal = &#123;Communications in Computer and Information Science&#125;,
&nbsp;&nbsp;volume = &#123;1342&#125;,
&nbsp;&nbsp;pages = &#123;282-295&#125;,
&nbsp;&nbsp;publisher = &#123;Southern African Conference for Artificial Intelligence Research&#125;,
&nbsp;&nbsp;isbn = &#123;978-3-030-66151-9&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.1007/978-3-030-66151-9_18&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-486">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Mouton C, Myburgh JC, Davel MH. Stride and translation invariance in CNNs. <i>Communications in Computer and Information Science </i>. 2020;1342. doi:https://doi.org/10.1007/978-3-030-66151-9_17.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-486" role="button" aria-expanded="false" aria-controls="collapseAbstract-486">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-486" role="button" aria-expanded="false" aria-controls="collapseBibtext-486">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-486-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-486" role="button" aria-expanded="false" aria-controls="collapsePdf-486">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-486" id="collapsePdf-486">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2021-04/SlideAndTranslationInvariance_Mouton.pdf" type="application/pdf">SlideAndTranslationInvariance_Mouton.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-486" id="collapseAbstract-486">
        <div class="p-3 border-left-grey mt-2">
            <p>Convolutional Neural Networks have become the standard for image classification tasks, however, these architectures are not invariant to translations of the input image. This lack of invariance is attributed to the use of stride which subsamples the input, resulting in a loss of information, and fully connected layers which lack spatial reasoning. We show that stride can greatly benefit translation invariance given that it is combined with sufficient similarity between neighbouring pixels, a characteristic which we refer to as local homogeneity. We also observe that this characteristic is dataset-specific and dictates the relationship between pooling kernel size and stride required for translation invariance. Furthermore we find that a trade-off exists between generalization and translation invariance in the case of pooling kernel size, as larger kernel sizes lead to better invariance but poorer generalization. Finally we explore the efficacy of other solutions proposed, namely global average pooling, anti-aliasing, and data augmentation, both empirically and through the lens of local homogeneity.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-486" id="collapseBibtext-486">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;486,
&nbsp;&nbsp;author = &#123;Coenraad Mouton and Johannes Myburgh and Marelie Davel&#125;,
&nbsp;&nbsp;title = &#123;Stride and translation invariance in CNNs&#125;,
&nbsp;&nbsp;abstract = &#123;Convolutional Neural Networks have become the standard for image classification tasks, however, these architectures are not invariant to translations of the input image. This lack of invariance is attributed to the use of stride which subsamples the input, resulting in a loss of information, and fully connected layers which lack spatial reasoning. We show that stride can greatly benefit translation invariance given that it is combined with sufficient similarity between neighbouring pixels, a characteristic which we refer to as local homogeneity. We also observe that this characteristic is dataset-specific and dictates the relationship between pooling kernel size and stride required for translation invariance. Furthermore we find that a trade-off exists between generalization and translation invariance in the case of pooling kernel size, as larger kernel sizes lead to better invariance but poorer generalization. Finally we explore the efficacy of other solutions proposed, namely global average pooling, anti-aliasing, and data augmentation, both empirically and through the lens of local homogeneity.&#125;,
&nbsp;&nbsp;year = &#123;2020&#125;,
&nbsp;&nbsp;journal = &#123;Communications in Computer and Information Science&#125;,
&nbsp;&nbsp;volume = &#123;1342&#125;,
&nbsp;&nbsp;pages = &#123;267-281&#125;,
&nbsp;&nbsp;publisher = &#123;Southern African Conference for Artificial Intelligence Research&#125;,
&nbsp;&nbsp;address = &#123;South Africa&#125;,
&nbsp;&nbsp;isbn = &#123;978-3-030-66151-9&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.1007/978-3-030-66151-9_17&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-394">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Theunissen MW, Davel MH, Barnard E. Benign interpolation of noise in deep learning. <i>South African Computer Journal</i>. 2020;32(2). doi:https://doi.org/10.18489/sacj.v32i2.833.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-394" role="button" aria-expanded="false" aria-controls="collapseAbstract-394">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-394" role="button" aria-expanded="false" aria-controls="collapseBibtext-394">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-394-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-394" role="button" aria-expanded="false" aria-controls="collapsePdf-394">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-394" id="collapsePdf-394">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2021-04/Benign-interpolation-theunissen.pdf" type="application/pdf">Benign-interpolation-theunissen.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-394" id="collapseAbstract-394">
        <div class="p-3 border-left-grey mt-2">
            <p>The understanding of generalisation in machine learning is in a state of flux, in part due to the ability of deep learning models to interpolate noisy training data and still perform appropriately on out-of-sample data, thereby contradicting long-held intuitions about the bias-variance trade off in learning. We expand upon relevant existing work by discussing local attributes of neural network training within the context of a relatively simple framework.We describe how various types of noise can be compensated for within the proposed framework in order to allow the deep learning model to generalise in spite of interpolating spurious function descriptors. Empirically,we support our postulates with experiments involving overparameterised multilayer perceptrons and controlled training data noise. The main insights are that deep learning models are optimised for training data modularly, with different regions in the function space dedicated to fitting distinct types of sample information. Additionally,we show that models tend to fit uncorrupted samples first. Based on this finding, we propose a conjecture to explain an observed instance of the epoch-wise double-descent phenomenon. Our findings suggest that the notion of model capacity needs to be modified to consider the distributed way training data is fitted across sub-units.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-394" id="collapseBibtext-394">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;394,
&nbsp;&nbsp;author = &#123;Marthinus Theunissen and Marelie Davel and Etienne Barnard&#125;,
&nbsp;&nbsp;title = &#123;Benign interpolation of noise in deep learning&#125;,
&nbsp;&nbsp;abstract = &#123;The understanding of generalisation in machine learning is in a state of flux, in part due to the ability of deep learning models to interpolate noisy training data and still perform appropriately on out-of-sample data, thereby contradicting long-held intuitions about the bias-variance trade off in learning. We expand upon relevant existing work by discussing local attributes of neural network training within the context of a relatively simple framework.We describe how various types of noise can be compensated for within the proposed framework in order to allow the deep learning model to generalise in spite of interpolating spurious function descriptors. Empirically,we support our postulates with experiments involving overparameterised multilayer perceptrons and controlled training data noise. The main insights are that deep learning models are optimised for training data modularly, with different regions in the function space dedicated to fitting distinct types of sample information. Additionally,we show that models tend to fit uncorrupted samples first. Based on this finding, we propose a conjecture to explain an observed instance of the epoch-wise double-descent phenomenon. Our findings suggest that the notion of model capacity needs to be modified to consider the distributed way training data is fitted across sub-units.&#125;,
&nbsp;&nbsp;year = &#123;2020&#125;,
&nbsp;&nbsp;journal = &#123;South African Computer Journal&#125;,
&nbsp;&nbsp;volume = &#123;32&#125;,
&nbsp;&nbsp;pages = &#123;80-101&#125;,
&nbsp;&nbsp;issue = &#123;2&#125;,
&nbsp;&nbsp;publisher = &#123;South African Institute of Computer Scientists and Information Technologists&#125;,
&nbsp;&nbsp;isbn = &#123;ISSN: 1015-7999; E:2313-7835&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.18489/sacj.v32i2.833&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-392">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Beukes JP, Davel MH, Lotz S. Pairwise networks for feature ranking of a geomagnetic storm model. <i>South African Computer Journal</i>. 2020;32(2). doi:https://doi.org/10.18489/sacj.v32i2.860.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-392" role="button" aria-expanded="false" aria-controls="collapseAbstract-392">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-392" role="button" aria-expanded="false" aria-controls="collapseBibtext-392">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-392-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-392" role="button" aria-expanded="false" aria-controls="collapsePdf-392">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-392" id="collapsePdf-392">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2021-04/pairwise-networks-beukes.pdf" type="application/pdf">pairwise-networks-beukes.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-392" id="collapseAbstract-392">
        <div class="p-3 border-left-grey mt-2">
            <p>Feedforward neural networks provide the basis for complex regression models that produce accurate predictions in a variety of applications. However, they generally do not explicitly provide any information about the utility of each of the input parameters in terms of their contribution to model accuracy. With this is mind, we develop the pairwise network, an adaptation to the fully connected feedforward network that allows the ranking of input parameters according to their contribution to the model output. The application is demonstrated in the context of a space physics problem. Geomagnetic storms are multi-day events characterised by significant perturbations to the magnetic field of the Earth, driven by solar activity. Previous storm forecasting efforts typically use solar wind measurements as input parameters to a regression problem tasked with predicting a perturbation index such as the 1-minute cadence symmetric-H (Sym-H) index. We re-visit the task of predicting Sym-H from solar wind parameters, with two &#039;twists&#039;: (i) Geomagnetic storm phase information is incorporated as model inputs and shown to increase prediction performance. (ii) We describe the pairwise network structure and training process - first validating ranking ability on synthetic data, before using the network to analyse the Sym-H problem.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-392" id="collapseBibtext-392">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;392,
&nbsp;&nbsp;author = &#123;Jacques Beukes and Marelie Davel and Stefan Lotz&#125;,
&nbsp;&nbsp;title = &#123;Pairwise networks for feature ranking of a geomagnetic storm model&#125;,
&nbsp;&nbsp;abstract = &#123;Feedforward neural networks provide the basis for complex regression models that produce accurate predictions in a variety of applications. However, they generally do not explicitly provide any information about the utility of each of the input parameters in terms of their contribution to model accuracy. With this is mind, we develop the pairwise network, an adaptation to the fully connected feedforward network that allows the ranking of input parameters according to their contribution to the model output. The application is demonstrated in the context of a space physics problem. Geomagnetic storms are multi-day events characterised by significant perturbations to the magnetic field of the Earth, driven by solar activity. Previous storm forecasting efforts typically use solar wind measurements as input parameters to a regression problem tasked with predicting a perturbation index such as the 1-minute cadence symmetric-H (Sym-H) index. We re-visit the task of predicting Sym-H from solar wind parameters, with two &amp;#039;twists&amp;#039;: (i) Geomagnetic storm phase information is incorporated as model inputs and shown to increase prediction performance. (ii) We describe the pairwise network structure and training process - first validating ranking ability on synthetic data, before using the network to analyse the Sym-H problem.&#125;,
&nbsp;&nbsp;year = &#123;2020&#125;,
&nbsp;&nbsp;journal = &#123;South African Computer Journal&#125;,
&nbsp;&nbsp;volume = &#123;32&#125;,
&nbsp;&nbsp;pages = &#123;35-55&#125;,
&nbsp;&nbsp;issue = &#123;2&#125;,
&nbsp;&nbsp;publisher = &#123;South African Institute of Computer Scientists and Information Technologists&#125;,
&nbsp;&nbsp;isbn = &#123;ISSN: 1015-7999; E:2313-7835&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.18489/sacj.v32i2.860&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">
    
<div class="mb-20" id="row-391">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Davel MH. Using summary layers to probe neural network behaviour. <i>South African Computer Journal</i>. 2020;32(2). doi:https://doi.org/10.18489/sacj.v32i2.861.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-391" role="button" aria-expanded="false" aria-controls="collapseAbstract-391">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-391" role="button" aria-expanded="false" aria-controls="collapseBibtext-391">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-391-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-391" role="button" aria-expanded="false" aria-controls="collapsePdf-391">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-391" id="collapsePdf-391">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2021-04/summary-nodes-davel.pdf" type="application/pdf">summary-nodes-davel.pdf</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-391" id="collapseAbstract-391">
        <div class="p-3 border-left-grey mt-2">
            <p>No framework exists that can explain and predict the generalisation ability of deep neural networks in general circumstances. In fact, this question has not been answered for some of the least complicated of neural network architectures: fully-connected feedforward networks with rectified linear activations and a limited number of hidden layers. For such an architecture, we show how adding a summary layer to the network makes it more amenable to analysis, and allows us to define the conditions that are required to guarantee that a set of samples will all be classified correctly. This process does not describe the generalisation behaviour of these networks,but produces a number of metrics that are useful for probing their learning and generalisation behaviour. We support the analytical conclusions with empirical results, both to confirm that the mathematical guarantees hold in practice, and to demonstrate the use of the analysis process.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-391" id="collapseBibtext-391">
<div class="p-3 border-left-grey mt-2">
<pre>@article&#123;391,
&nbsp;&nbsp;author = &#123;Marelie Davel&#125;,
&nbsp;&nbsp;title = &#123;Using summary layers to probe neural network behaviour&#125;,
&nbsp;&nbsp;abstract = &#123;No framework exists that can explain and predict the generalisation ability of deep neural networks in general circumstances. In fact, this question has not been answered for some of the least complicated of neural network architectures: fully-connected feedforward networks with rectified linear activations and a limited number of hidden layers. For such an architecture, we show how adding a summary layer to the network makes it more amenable to analysis, and allows us to define the conditions that are required to guarantee that a set of samples will all be classified correctly. This process does not describe the generalisation behaviour of these networks,but produces a number of metrics that are useful for probing their learning and generalisation behaviour. We support the analytical conclusions with empirical results, both to confirm that the mathematical guarantees hold in practice, and to demonstrate the use of the analysis process.&#125;,
&nbsp;&nbsp;year = &#123;2020&#125;,
&nbsp;&nbsp;journal = &#123;South African Computer Journal&#125;,
&nbsp;&nbsp;volume = &#123;32&#125;,
&nbsp;&nbsp;pages = &#123;102-123&#125;,
&nbsp;&nbsp;issue = &#123;2&#125;,
&nbsp;&nbsp;publisher = &#123;South African Institute of Computer Scientists and Information Technologists&#125;,
&nbsp;&nbsp;isbn = &#123;ISSN: 1015-7999; E:2313-7835&#125;,
&nbsp;&nbsp;url = &#123;http://hdl.handle.net/10394/36916&#125;,
&nbsp;&nbsp;doi = &#123;https://doi.org/10.18489/sacj.v32i2.861&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
    <div class="w-100 views-row">

<div class="mb-20" id="row-236">
    <div class="bibcite-citation">
      <div class="csl-bib-body">
  <div class="csl-entry"><div class="csl-left-margin">1.</div><div class="csl-right-inline">Davel MH, Theunissen MW, Pretorius AP, Barnard E. DNNs as layers of cooperating classifiers. In: <i>The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)</i>. New York; 2020.</div></div>
</div>
  </div>

    
    <div class="w-100 mt-2">
        <a class="abstract mr-2" data-toggle="collapse" href="#collapseAbstract-236" role="button" aria-expanded="false" aria-controls="collapseAbstract-236">
           <i class="fas fa-angle-right icon-orange"></i> Abstract
        </a>
        <a class="bibtext mr-2" data-toggle="collapse" href="#collapseBibtext-236" role="button" aria-expanded="false" aria-controls="collapseBibtext-236">
           <i class="fas fa-angle-right icon-orange"></i> BibTex Entry
        </a>
        <a class="bibtext-export mr-2" href="../bibcite/export/bibtex/bibcite_reference/bibcite_reference-236-BibTeX.bib"><i class="fas fa-angle-right icon-orange"></i> BibTeX Download
        </a>
                    <a class="pdf-files mr-2" data-toggle="collapse" href="#collapsePdf-236" role="button" aria-expanded="false" aria-controls="collapsePdf-236">
               <i class="fas fa-file-pdf icon-orange"></i> PDF
            </a>
                  
    </div>

    <div class="collapse" data-parent="#row-236" id="collapsePdf-236">
         <div class="p-3 border-left-grey mt-2 listing-files">
            
<span class="file file--mime-application-pdf file--application-pdf"> <a href="../sites/default/files/2020-04/davel-2020-dnns-layers-classifiers.pdf" type="application/pdf" title="davel-2020-dnns-layers-classifiers.pdf">Published (shortened) version</a></span>

            
            
        </div>
    </div>

    <div class="collapse" data-parent="#row-236" id="collapseAbstract-236">
        <div class="p-3 border-left-grey mt-2">
            <p>A robust theoretical framework that can describe and predict the generalization ability of deep neural networks (DNNs) in general circumstances remains elusive. Classical attempts have produced complexity metrics that rely heavily on global measures of compactness and capacity with little investigation into the effects of sub-component collaboration. We demonstrate intriguing regularities in the activation patterns of the hidden nodes within fully-connected feedforward networks. By tracing the origin of these patterns, we show how such networks can be viewed as the combination of two information processing systems: one continuous and one discrete. We describe how these two systems arise naturally from the gradient-based optimization process, and demonstrate the classification ability of the two systems, individually and in collaboration. This perspective on DNN classification offers a novel way to think about generalization, in which different subsets of the training data are used to train distinct classifiers; those classifiers are then combined to perform the classification task, and their consistency is crucial for accurate classification.</p>
        </div>
    </div>

    <div class="collapse" data-parent="#row-236" id="collapseBibtext-236">
<div class="p-3 border-left-grey mt-2">
<pre>@&#123;236,
&nbsp;&nbsp;author = &#123;Marelie Davel and Marthinus Theunissen and Arnold Pretorius and Etienne Barnard&#125;,
&nbsp;&nbsp;title = &#123;DNNs as layers of cooperating classifiers&#125;,
&nbsp;&nbsp;abstract = &#123;A robust theoretical framework that can describe and predict the generalization ability of deep neural networks (DNNs) in general circumstances remains elusive. Classical attempts have produced complexity metrics that rely heavily on global measures of compactness and capacity with little investigation into the effects of sub-component collaboration. We demonstrate intriguing regularities in the activation patterns of the hidden nodes within fully-connected feedforward networks. By tracing the origin of these patterns, we show how such networks can be viewed as the combination of two information processing systems: one continuous and one discrete. We describe how these two systems arise naturally from the gradient-based optimization process, and demonstrate the classification ability of the two systems, individually and in collaboration. This perspective on DNN classification offers a novel way to think about generalization, in which different subsets of the training data are used to train distinct classifiers; those classifiers are then combined to perform the classification task, and their consistency is crucial for accurate classification.&#125;,
&nbsp;&nbsp;year = &#123;2020&#125;,
&nbsp;&nbsp;journal = &#123;The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)&#125;,
&nbsp;&nbsp;pages = &#123;3725 - 3732&#125;,
&nbsp;&nbsp;month = &#123;07/02-12/02/2020&#125;,
&nbsp;&nbsp;address = &#123;New York&#125;,
&#125;
</pre>
</div>
</div>
</div>

</div>
</div>

    </div>
  
        <nav aria-label="pagination-heading">
    <h4 id="pagination-heading" class="sr-only">Pagination</h4>
    <ul class="pagination js-pager__items">
                                                        <li class="page-item active">
                      <span class="page-link">1</span>
                  </li>
              <li class="page-item ">
                      <a href="54-1.html?page=1" title="" class="page-link">2</a>
                  </li>
              <li class="page-item ">
                      <a href="54-2.html?page=2" title="" class="page-link">3</a>
                  </li>
                                      <li class="pager__item--next">
          <a href="54-1.html?page=1" title="Go to next page" rel="next" class="page-link">
            <span aria-hidden="true">Next ›</span>
            <span class="sr-only">Next page</span>
          </a>
        </li>
                          <li class="page-item">
          <a href="54-2.html?page=2" title="Go to last page" class="page-link">
            <span aria-hidden="true">Last »</span>
            <span class="sr-only">Last page</span>
          </a>
        </li>
          </ul>
  </nav>

          </div>
</div>

    </div>
  </div>

</div>
            <div class="col-auto"></div>
          </div>
        </aside>
        <aside class="container clearfix" role="complementary">
          
          
          
        </aside>
      </div>
          <div class="d-flex w-100">
        <section class="region region-brands">
    <div id="block-companylogos" class="block block-block-content block-block-content4f032544-7aeb-42fc-a5c4-5689cc32dcac">
  
    
      <div class="content">
      
            <div class="clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item"><ul><li><a href="https://www.csir.co.za" target="_blank"><img alt="CSIR" data-entity-type="file" data-entity-uuid="3488e975-cefd-43d0-833f-c7c966044a41" src="../sites/default/files/inline-images/csir_logo_wide.png" width="592" height="180" loading="lazy"></a></li>
	<li><a href="https://www.dst.gov.za" target="_blank"><img alt="DSI" data-entity-type="file" data-entity-uuid="8297cef2-3f93-42ac-9f41-306426e612ba" src="../sites/default/files/inline-images/dsi_logo.png" width="853" height="260" loading="lazy"></a></li>
	<li><a href="https://sacoronavirus.co.za" target="_blank"><img alt="Covid-19" data-entity-type="file" data-entity-uuid="fb53689c-66c1-4b22-a659-3da75ebc644b" src="../sites/default/files/inline-images/covid-footer_0.png" class="align-right" width="900" height="260" loading="lazy"></a></li>
</ul></div>
      
    </div>
  </div>

  </section>

    </div>
     <footer class="site-footer pt-5 pb-6">
              <div class="container">
                      <div class="site-footer__top clearfix">
                <section class="row region region-footer-first">
    <nav role="navigation" aria-labelledby="block-cair-sass-footer-menu" id="block-cair-sass-footer" class="block block-menu navigation menu--footer">
            
  <h2 class="sr-only" id="block-cair-sass-footer-menu">Footer menu</h2>
  

        
              <ul block="block-cair-sass-footer" class="clearfix nav navbar-nav">
                    <li class="nav-item">
                <a href="../events.html" class="nav-link nav-link--events" data-drupal-link-system-path="node/255">Events</a>
              </li>
                <li class="nav-item">
                <a href="../news.html" class="nav-link nav-link--news" data-drupal-link-system-path="node/254">News</a>
              </li>
                <li class="nav-item">
                <a href="../about.html" class="nav-link nav-link--about" data-drupal-link-system-path="node/4">About</a>
              </li>
                <li class="nav-item">
                <a href="../contact.html" class="nav-link nav-link--contact" data-drupal-link-system-path="webform/contact">Contact</a>
              </li>
        </ul>
  


  </nav>
<nav role="navigation" aria-labelledby="block-cair-sass-account-menu-menu" id="block-cair-sass-account-menu" class="block block-menu navigation menu--account">
            
  <h2 class="sr-only" id="block-cair-sass-account-menu-menu">User account menu</h2>
  

        
              <div block="block-cair-sass-account-menu" class="clearfix nav">
                      <a href="login.html" class="nav-link nav-link--user-login" data-drupal-link-system-path="user/login">Log in</a>
            </div>
      


  </nav>

  </section>

            </div>
                            </div>
          </footer>
  </div>
</div>

  </div>

    
    <script type="application/json" data-drupal-selector="drupal-settings-json">{"path":{"baseUrl":"\/","scriptPath":null,"pathPrefix":"","currentPath":"user\/54","currentPathIsAdmin":false,"isFront":false,"currentLanguage":"en"},"pluralDelimiter":"\u0003","suppressDeprecationErrors":true,"ajaxPageState":{"libraries":"addtoany\/addtoany,blazy\/bio.ajax,bootstrap_barrio\/breadcrumb,bootstrap_barrio\/form,bootstrap_barrio\/global-styling,cair_sass\/global-styling,particlesjs\/particlesjs,particlesjs_cair\/particlesjs_cair,system\/base,views\/views.ajax,views\/views.module","theme":"cair_sass","theme_token":null},"ajaxTrustedUrl":{"\/search":true},"views":{"ajax_path":"\/views\/ajax","ajaxViews":{"views_dom_id:f843f2bbb0cbc8f7c59a20ed5ca5da5bf383e309aea77bed48a3aebbf98c560a":{"view_name":"research_papers","view_display_id":"block_2","view_args":"54","view_path":"\/user\/54","view_base_path":null,"view_dom_id":"f843f2bbb0cbc8f7c59a20ed5ca5da5bf383e309aea77bed48a3aebbf98c560a","pager_element":0}}},"user":{"uid":0,"permissionsHash":"0894c071b06b3d56de3346155916aa642009fcc0d2ed55dbcc6226fa6ed06924"}}</script>
<script src="../core/assets/vendor/jquery/jquery.min.js?v=3.6.3"></script>
<script src="../core/misc/polyfills/element.matches.js?v=9.5.8"></script>
<script src="../core/misc/polyfills/object.assign.js?v=9.5.8"></script>
<script src="../core/misc/polyfills/nodelist.foreach.js?v=9.5.8"></script>
<script src="../core/assets/vendor/css-escape/css.escape.js?v=1.5.1"></script>
<script src="../core/assets/vendor/es6-promise/es6-promise.auto.min.js?v=4.2.8"></script>
<script src="../core/assets/vendor/once/once.min.js?v=1.0.1"></script>
<script src="../core/assets/vendor/jquery-once/jquery.once.min.js?v=2.2.3"></script>
<script src="../core/misc/drupalSettingsLoader.js?v=9.5.8"></script>
<script src="../core/misc/drupal.js?v=9.5.8"></script>
<script src="../core/misc/drupal.init.js?v=9.5.8"></script>
<script src="../core/misc/debounce.js?v=9.5.8"></script>
<script src="../modules/contrib/blazy/js/dblazy.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/plugin/blazy.dataset.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/plugin/blazy.viewport.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/plugin/blazy.dom.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/plugin/blazy.xlazy.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/plugin/blazy.observer.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/base/blazy.base.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/base/io/bio.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/base/io/bio.media.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/base/blazy.drupal.min.js?s9x51z"></script>
<script src="../modules/contrib/blazy/js/blazy.compat.min.js?s9x51z"></script>
<script src="../core/assets/vendor/tabbable/index.umd.min.js?v=5.3.3"></script>
<script src="../menu/page.js" async=""></script>
<script src="../modules/contrib/addtoany/js/addtoany.js?v=9.5.8"></script>
<script src="../core/misc/jquery.once.bc.js?v=9.5.8"></script>
<script src="../themes/custom/cair_sass/js/popper.min.js?v=9.5.8"></script>
<script src="../themes/custom/cair_sass/js/bootstrap.min.js?v=9.5.8"></script>
<script src="../themes/custom/cair_sass/js/jquery.matchHeight-min.js?v=9.5.8"></script>
<script src="../themes/custom/cair_sass/js/theme.js?v=9.5.8"></script>
<script src="../themes/custom/cair_sass/js/global.js?v=9.5.8"></script>
<script src="../libraries/particles.js/particles.js?v=2.0.0"></script>
<script src="../modules/custom/particlesjs/particlesjs_cair/js/particlesjs_cair.js?v=1.x"></script>
<script src="../core/misc/progress.js?v=9.5.8"></script>
<script src="../core/assets/vendor/loadjs/loadjs.min.js?v=4.2.0"></script>
<script src="../core/themes/claro/js/ajax.js?s9x51z"></script>
<script src="../core/misc/ajax.js?v=9.5.8"></script>
<script src="../modules/contrib/blazy/js/base/io/bio.ajax.min.js?s9x51z"></script>
<script src="../core/assets/vendor/jquery-form/jquery.form.min.js?v=4.3.0"></script>
<script src="../core/modules/views/js/base.js?v=9.5.8"></script>
<script src="../core/modules/views/js/ajax_view.js?v=9.5.8"></script>
<script src="../themes/contrib/bootstrap_barrio/js/modules/views/ajax_view.js?s9x51z"></script>

  </body>
</html>
